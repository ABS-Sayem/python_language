{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = \"F:/courses/mlds_nactar/dataset/iris.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "#df.head()\n",
    "\n",
    "# Map Classes\n",
    "classes = {\n",
    "    'setosa': 0,\n",
    "    'versicolor': 1,\n",
    "    'virginica': 2\n",
    "}\n",
    "df['species'] = df['species'].map(classes)\n",
    "#df.tail()\n",
    "#print(df['species'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Additional Link`:\n",
    "* [MLXtend](http://rasbt.github.io/mlxtend/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y = df['species'].values\n",
    "X = df.iloc[:, 1:5].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`processing a large data file iteratively`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_csv = \"large_file.csv\"\n",
    "chunk_size = 100000     #[number of lines to process at each iteration]\n",
    "\n",
    "# specify the columns that should be read\n",
    "columns = ['column0', 'column1', 'column3', 'column7', 'column9',]\n",
    "\n",
    "# get the number of lines in csv file\n",
    "nlines = subprocess.check_output(['wc', '-l', in_csv])\n",
    "nlines = int(nlines.split()[0])\n",
    "\n",
    "# iteratively read csv and dump lines into the SQLite table\n",
    "for i in range(0, nlines, chunk_size):  # change 0 --> 1 if the dataset contains a column header\n",
    "    df = pd.read_csv(in_csv,\n",
    "                     header = None,     # no header, define column header later manually\n",
    "                     nrows = chunk_size,    # number of rows to read at each iteration\n",
    "                     skiprows = i   # skip rows that are already read\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Modin`: data handler like pandas, actually a customization of pandas, sent data in chunk rather than as a whole in pandas. [Github](https://github.com/modin-project/modin)\n",
    "* Installation: `pip install modin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modin.pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "frame_data = np.random.randint(0, 100, size=(2**10, 2**8))\n",
    "df = pd.DataFrame(frame_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dask`: [Github](https://github.com/dask/dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress\n",
    "client = Client(n_workers=2, threads_per_worker=2, memory_limit='1GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "df = dask.datasets.timeseries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifier(object):\n",
    "    import numpy as np\n",
    "    def __init__(self, k, dist_fn=None):\n",
    "        self.k = k\n",
    "        if dist_fn is None: self.dist_fn = _euclidean_dist()\n",
    "    \n",
    "    def _euclidean_dist(self, a, b):\n",
    "        dist = 0\n",
    "        for ele_i, ele_j in zip(a,b): dist += ((ele_i - ele_j)**2)\n",
    "        dist = dist**0.5\n",
    "        return dist\n",
    "    \n",
    "    def _find_nearest(self, x):\n",
    "        dist_idx_pairs = []\n",
    "        for j in range(self.dataset_.shape[0]):\n",
    "            d = self.dist_fn(x, self.dataset_[j])\n",
    "            dist_idx_pairs.append((d,j))\n",
    "        sorted_dist_idx_pairs = sorted(dist_idx_pairs)\n",
    "        return sorted_dist_idx_pairs\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.dataset_ = X.copy()\n",
    "        self.labels_ = y.copy()\n",
    "        self.possible_labels_ = np.unique(y)\n",
    "        # the tailing underscore is a scikit-learn convention, means- thay are only available for this functions only.\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(X.shape[0], dtype=int)\n",
    "        for i in range(X.shape[0]):\n",
    "            k_nearest = self._find_nearest(X[i])[:self.k]\n",
    "            indices = [entry[1] for entry in k_nearest]\n",
    "            k_labels = self.labels_[indices]\n",
    "            counts = np.bincount(k_labels, minlength=self.possible_labels_.shape[0])\n",
    "            pred_label = np.argmax(counts)\n",
    "            predictions[i] = pred_label\n",
    "            return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training ...\")\n",
    "knn_model = KNNClassifier(k=3)\n",
    "knn_model.fit(X_train, y_train)\n",
    "print(f\"Training Done\")\n",
    "#print(knn_model.predict(X_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
